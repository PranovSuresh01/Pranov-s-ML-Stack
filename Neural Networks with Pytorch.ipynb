{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2f8c7bb-3824-4ca6-a289-bbf507133fbe",
   "metadata": {},
   "source": [
    "# ECON 425 Homework 6 - Neural Networks\n",
    "## Submission by Pranov Suresh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7b1e06-5589-4e4f-8bfb-b7fb83d077e7",
   "metadata": {},
   "source": [
    "## Q 1 - Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0821acd8-1c00-4193-8546-d4f8471ef1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from scipy.optimize import minimize\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5925ad23-350a-4c03-a8e6-82272f0044c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 1000000\n",
    "np.random.seed(11)\n",
    "x = np.random.normal(0,1, sample_size)\n",
    "def sigmoid(z):\n",
    "   return 1/(1 + np.exp(-z))\n",
    "y = sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e1140de-6da1-4da7-93ff-9761b2b1b7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network_2(weights, x, y):\n",
    "    w0, w1, w2, = weights\n",
    "    h1 = w0*x\n",
    "    z1 = sigmoid(h1)\n",
    "    h2 = np.dot(w1,z1)\n",
    "    z2 = sigmoid(h2)\n",
    "    y_hat = np.dot(z2, w2)\n",
    "    loss = np.mean((y_hat - y) ** 2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97a996f4-b19b-48ad-8ff1-bcc7cfa270f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network_1(weights, x, y):\n",
    "    w0, w1 = weights\n",
    "    h1 = x*w0\n",
    "    z1 = sigmoid(h1)\n",
    "    y_hat = w1 * z1\n",
    "    loss = np.mean((y_hat-y)**2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48335c2d-00f1-4094-a1f1-9186e48fcff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weights_one = np.array([0.5, 0.6])\n",
    "sample_weights_two = np.array([0.5,0.75, 0.35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2e38edb-106f-4d86-a7b5-228a6d404198",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_one = neural_network_1(sample_weights_one, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8dc37074-8871-488c-82d4-25b0faa65b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_two = neural_network_2(sample_weights_two, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a7d0844-f4f6-41bb-bf2a-1fe8e23b191b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimised_weights_one = minimize(neural_network_1, sample_weights_one, args = (x,y), method = \"BFGS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ae5e11f-c03c-418e-9b48-2c69c0540f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimised_weights_two = minimize(neural_network_2, sample_weights_two, args=(x, y), method = \"BFGS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff32022f-6051-4f2f-adf3-e36dfbfa89b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network 1:\n",
      "Initial Error: 0.05895706909518932\n",
      "Final Error: 1.92841113648809e-10\n",
      "Optimized Weights: [0.99990374 1.00001395]\n",
      "\n",
      "Neural Network 2:\n",
      "Initial Error: 0.12591543999922\n",
      "Final Error: 0.010062587343962363\n",
      "Optimized Weights: [5.59552035 3.45803255 0.67314698]\n"
     ]
    }
   ],
   "source": [
    "final_error_one = optimised_weights_one.fun\n",
    "final_error_two = optimised_weights_two.fun\n",
    "\n",
    "print(\"Neural Network 1:\")\n",
    "print(f\"Initial Error: {test_results_one}\")\n",
    "print(f\"Final Error: {final_error_one}\")\n",
    "print(f\"Optimized Weights: {optimised_weights_one.x}\")\n",
    "\n",
    "print(\"\\nNeural Network 2:\")\n",
    "print(f\"Initial Error: {test_results_two}\")\n",
    "print(f\"Final Error: {final_error_two}\")\n",
    "print(f\"Optimized Weights: {optimised_weights_two.x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f2dfc1-8c47-492b-a90d-dbf6ca4df41c",
   "metadata": {},
   "source": [
    "## Having built the neural network with one and two layers respectively, we can see that the training error of the models, using the optimised weights noticeably increases. This increase is also noticed when the model is built on randomly guessed weights initially. \n",
    "\n",
    "### The initial error for the one layer neural network yields 0.058595, while the inital error for the two layer neural network yields 0.12591 already revealing the pattern which we were looking for. The final error using optimised weights rises from 0.00000000019284 to 0.0100625. This is a larger increase than even in the initial random error. \n",
    "\n",
    "Given the relatively simple structure of the first model, we might be possibly looking at a case of model overfitting in the first case, as training error is not reflective of testing accuracy. Even if we were to assume that the first model is quite robust, the training error increase is explainable by the increased complexity of the model. One with more hidden layers and neutrons might possibly lead to more variance, as part of the standard complexity-variance tradeoff. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e3734b-1e9d-4799-81a2-42de1c1c8ff0",
   "metadata": {},
   "source": [
    "## Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28066b20-6866-45a7-897f-465f91ecb27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_data = pd.read_csv(\"E:/UCLA/Winter 2024/ECON 425/card_transdata.csv\")\n",
    "X = pd.DataFrame(credit_card_data.drop(\"fraud\",axis = 1))\n",
    "y = pd.DataFrame(credit_card_data[\"fraud\"])\n",
    "X_train = X.iloc[:500000]\n",
    "y_train = y.iloc[:500000]\n",
    "X_test = X.iloc[500000:]\n",
    "y_test = y.iloc[500000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6597405d-2bed-4748-bbe5-0dec89e68e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train.values)\n",
    "X_test_tensor = torch.tensor(X_test.values)\n",
    "y_train_tensor = torch.tensor(y_train.values).squeeze().long()\n",
    "y_test_tensor = torch.tensor(y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cf63c4f-389f-483f-a9a9-7045d65144fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1,output_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size1, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1974b41c-af9b-4f16-9cf0-8311ace149eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "input_size = X_train_tensor.shape[1]\n",
    "hidden_size1 = 150\n",
    "output_size = 2\n",
    "model = NeuralNetwork(input_size, hidden_size1, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ba7f02a-9317-430b-a81e-c8b1479aad05",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.01  \n",
    "optimiser = optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95b9dd01-507f-48ac-8c2e-6aa519477d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs =15  \n",
    "batch_size = 128\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c90ed42e-7a20-47aa-a6a8-8a16de5ba8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_criterion, optimiser):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (x,y) in enumerate(dataloader):\n",
    "        optimiser.zero_grad()\n",
    "        x = x.float()\n",
    "        outputs = model(x)\n",
    "        loss = loss_criterion(outputs, y)  \n",
    "        loss.backward()  \n",
    "        optimiser.step()\n",
    "        if batch % 100 == 0: \n",
    "            loss, current = loss.item(), batch*batch_size + len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd0d7d19-35ae-4b55-9240-540f2f048fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "----------\n",
      "Epoch 2\n",
      "----------\n",
      "Epoch 3\n",
      "----------\n",
      "Epoch 4\n",
      "----------\n",
      "Epoch 5\n",
      "----------\n",
      "Epoch 6\n",
      "----------\n",
      "Epoch 7\n",
      "----------\n",
      "Epoch 8\n",
      "----------\n",
      "Epoch 9\n",
      "----------\n",
      "Epoch 10\n",
      "----------\n",
      "Epoch 11\n",
      "----------\n",
      "Epoch 12\n",
      "----------\n",
      "Epoch 13\n",
      "----------\n",
      "Epoch 14\n",
      "----------\n",
      "Epoch 15\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}\\n----------\")\n",
    "    train_loop(train_dataloader, model, loss_criterion, optimiser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc3db27d-4960-4c14-abf4-6d917a02a46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8672\n",
      "Test F1 Score: 0.0693\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "model.eval()\n",
    "predicted_values = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dataloader: \n",
    "        outputs = model(inputs.float())\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        predicted_values.extend(predictions.tolist())\n",
    "\n",
    "accuracy = accuracy_score(y_test_tensor.numpy(), predicted_values)\n",
    "f1 = f1_score(y_test_tensor.numpy(), predicted_values)\n",
    "\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n",
    "print(f'Test F1 Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7af2ac7-e01b-4ca1-8063-c920e78a8792",
   "metadata": {},
   "source": [
    "## Based on the selected parameters of 150 neurons, 20 epochs, and a batch size of 128, we find that the testing F1-score is 0.0706. This is comparable to a F1-score of nearly 0.9999 for the decision tree models built on the same dataset. \n",
    "\n",
    "### Purely based on the testing F1-score, it is clear that the decision tree model fares much better than the neural network. \n",
    "\n",
    "The reasons behind this could be down to the standard model complexity-variance tradeoff, where the decision tree model performs better due to its simpler model structure. A model with 150 neurons, one hidden layer, and multiple epochs could lead to a poor testing F1-score. \n",
    "Thus, it can be concluded that the decision tree model is better and should be selected. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
